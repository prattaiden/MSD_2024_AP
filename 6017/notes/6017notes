5/14

data analysis and vis

jupyter notebooks
document broken into cells

python cell

pycharm

HOMEBREW THE INSTALL FOR PYTHON


5/15
hw 1 due 5/24
python general stuff

list comprehension

tuples
syntax to define is the ,
tuples get printed in ()


PANDA
series object
each entry in the series has as key and a value
if dont provide key, it is integer 0 -

5/21
intro to visualization

matplotlib

np.meshgrid(ax,ax)

5/22
test statistic

bernoili distribution

hypothesis testing
t test
z tests

5/23

box plot of all the values for HW1

linear regression
- predictions
assume that as change sqft a little price change a little

predict closest value nearest neighbor
best?
do my predictions match the data

prediction for particular point
bo bi xi

best line - arg min
find beta0 and beta1 - sum up expression over all data points

next try min |pred(xi) - yi|

min prediction(xi)- yi ^2 - least squares fit

residual sum of squares = error - regression - scale dependent

how can we get something that is not scale dependent
total sum of squares
TSS = su, (yi - mean of y)^2
relative error r2
tss - rss / tss = 1 - tss/tss = r2

if no error , r squared = 1
RSS <= TSS

linear regresiion, take data points, plug in to get betas and best lines

import statsmodels.formula.api as sm
from sklearn import lineaer_model

hy

5/28
more regression
y=B0 + B1X

difference between each point and the line at that point , add all errors

y = b0 + b1 x1 + b2 x2
if in a function add enough betas ,  can get line to match exactly, but that is not useful
more complicated models = high risk of over fitting
as add more complicated terms, yes boost r^2 value but not getting anything good out of it that helps with predictions
split data into trainign and testing, fit betas with training data, compute r^2 for both sets of the data

training error       testing error
          small       big
small     goal           overfit

big                     underfit

if you have overfit: remove terms - model is too complicated
underfit: add terms - probably

split into train/test : cross validation

6/4
KNN
predictive data analysis
reduce statsmodels
m(x) = y
knn for regresion - use averaging

point - traububg points
point k knn(point p, int k)
for tp: training points
d = dist(p, tp)

for p : ret if d < distance(pr,p)
replace pr with pt

6//6
spdt implementation
and homework 4
tree based data structures for spatial partitioning

quad tree
if too big , split bounding box into four peices

KD tree
split on median x point
then split each half by median y
repeat

implementation

K tree class
constructor -> make the root -> need a node class and constructor - recursive
node class , left and right children

split dimension
2D it is x or y
in KD it is an index used to look up the point

points class
priority queue
 access bounding boxes

quad tree node
  children are a little different - 4 children - NW NE SW SE
  point array []
  either an internal node and have children or a point array with points

implementing two tree data structures, compare performance under different conditions

base case for quad tree
D

dub run to run the main function

opBinary
operatory overloading
string version of op want to overload '

kd tree is also log N becsue it is perfecrly balanced as the points are split


6/11
for csv timing: K D then the timing

decision tree:
ISP customer service
performing classification
class == tree of connection issue

 effectively KD trees
 often features are categorical
 order looking at points is a bit different from kd tree
 only consider training data in one region

 DECISION TREE
 often but not necessearily binary
 internal nodes correspond to decisions
 leaf nodes are predictions

 is modem plugged in ? = root node
 - no = customer problem
 - yes - is router plugged in - yes - no

 often a binary tree, if a question has more than two answers , wont be

 leaf nodes are dtraining points that fit in the bucket
 leaf node contain number of trianing pooints in each class

 benefits
 unlike knn , no
 potentially shorter trees
 understandability

 use a good dt to understand which features are most important
 works well for categorical and herterogeneus features

 KD tree example
 calories and act score
 calories number range is large strip
 act score is small circles
 works for herterogeneus where scales are different
 and for categorical data

 how do we build a DT ?
 what kinds of training points in leaf nodes to give good prediction ?
 ideally leaf nodes are mostly a single class.
 internal nodes are important features

 root node and internal nodes split the data into classes as well as possible

 ID3 algorithm greedy
 greedy algorithms are generally cheaper than other algorithms
 whenever have to make a choice, pick what seems best , not neceessarily always

 set of training points and a set of features
 if # of training points is small or the number of features is 0
 then we make a leaf node
 else make an internal node
 for each feature, find the best split value for that feature
 and then make node using the best feature
 remove feature from last and creatre l/r children
 "best split"
  is if my children are going to be leafnodes , evaluate a split by pretending the children will be leafnodes

  evaluting leaf nodes
  sum of square residuals
  (prediction - the true)^2 E trianing points

  GINI
  measure of "inequality"
0 if all points have the same class , larger number means it is more heterogeneous
entropy - 0 for all same class
metrics measure how all the points are in the same class or not

very easy to built a decision tree with %100 training accuracy
if we get %100 it means we OVERFIT the data

improving accuracy and dealing with overfitting in general
least squares regression

data - smaples from some distribution

models are also samples from some distribution
usually we get one sample

bootstrapping turn 1 dat set into many

make a new data set
just take random values from original dataset but do it with replacement
making a pretend dataset by taking N samples from N random rows

use this with decision trees with a technique called bagigng

BAGGING
 bootstrap many datasets, train and overfit a decision tree on each one
 prediction is just a vote by all the decision trees
 bagging taking an average of the noisy prediction

other technique is RANDOM FOREST
generate a bunch of different decisions trees from one dataset by limitting which features we use to split
build many dts from one data set
randomly limit the set of features we could split on at each node

using voting to determine the final prediction

example
:

6/12
support vector machines
use for classification

decision boundary


line between points is a separating hyper plane

in 1D how to pick the best separating hyper plane

distance from hyperplane to closest training point is called the margin
in general, want maximum margin hyperplane

in 2d hyperplane is  line
maximum margin is more difficult
placement and angle determine the margin
equation of this line  use the normal vector, direction perpendicular to the surface
n * x + c =
nx * px + ny * py + c = 0
normal is always a vector, taking the points as vectors

error for blue point is 0 if on correct side of dotted blue line, distance to dotted blue line otherwise
may be some small penalty for correcrly classified points if they are within the width of the slab and close to the hyperplane

solve for decision boundary plane which minimizes the error

can solve for n , b - the plane offset

knn nore accurate
model size / prediction cost
   all training points plus acceleration structure

linear svc
normal vector and b
very compact model
small to store and predictions are cheap

downside is the linear decision boundary

topgraphical

if i consider the space to be one cluster sheet, ti can stretch into different dimensins then draw a straight line
kernel function
"kernel frick"

radiabl vase function - rvf kernel

linear svc -> support vector machine

k fold cross validation
