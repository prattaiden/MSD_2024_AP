5/14

data analysis and vis

jupyter notebooks
document broken into cells

python cell

pycharm

HOMEBREW THE INSTALL FOR PYTHON


5/15
hw 1 due 5/24
python general stuff

list comprehension

tuples
syntax to define is the ,
tuples get printed in ()


PANDA
series object
each entry in the series has as key and a value
if dont provide key, it is integer 0 -

5/21
intro to visualization

matplotlib

np.meshgrid(ax,ax)

5/22
test statistic

bernoili distribution

hypothesis testing
t test
z tests

5/23

box plot of all the values for HW1

linear regression
- predictions
assume that as change sqft a little price change a little

predict closest value nearest neighbor
best?
do my predictions match the data

prediction for particular point
bo bi xi

best line - arg min
find beta0 and beta1 - sum up expression over all data points

next try min |pred(xi) - yi|

min prediction(xi)- yi ^2 - least squares fit

residual sum of squares = error - regression - scale dependent

how can we get something that is not scale dependent
total sum of squares
TSS = su, (yi - mean of y)^2
relative error r2
tss - rss / tss = 1 - tss/tss = r2

if no error , r squared = 1
RSS <= TSS

linear regresiion, take data points, plug in to get betas and best lines

import statsmodels.formula.api as sm
from sklearn import lineaer_model

hy

5/28
more regression
y=B0 + B1X

difference between each point and the line at that point , add all errors

y = b0 + b1 x1 + b2 x2
if in a function add enough betas ,  can get line to match exactly, but that is not useful
more complicated models = high risk of over fitting
as add more complicated terms, yes boost r^2 value but not getting anything good out of it that helps with predictions
split data into trainign and testing, fit betas with training data, compute r^2 for both sets of the data

training error       testing error
          small       big
small     goal           overfit

big                     underfit

if you have overfit: remove terms - model is too complicated
underfit: add terms - probably

split into train/test : cross validation

6/4
KNN
predictive data analysis
reduce statsmodels
m(x) = y
knn for regresion - use averaging

point - traububg points
point k knn(point p, int k)
for tp: training points
d = dist(p, tp)

for p : ret if d < distance(pr,p)
replace pr with pt

6//6
spdt implementation
and homework 4
tree based data structures for spatial partitioning

quad tree
if too big , split bounding box into four peices

KD tree
split on median x point
then split each half by median y
repeat

implementation

K tree class
constructor -> make the root -> need a node class and constructor - recursive
node class , left and right children

split dimension
2D it is x or y
in KD it is an index used to look up the point

points class
priority queue
 access bounding boxes

quad tree node
  children are a little different - 4 children - NW NE SW SE
  point array []
  either an internal node and have children or a point array with points

implementing two tree data structures, compare performance under different conditions

base case for quad tree
D

dub run to run the main function

opBinary
operatory overloading
string version of op want to overload '

kd tree is also log N becsue it is perfecrly balanced as the points are split
